# SimShop: RL Recommender System with Simulated Users

**SimShop** is a research-oriented project that simulates user behavior in an e-commerce environment and trains a recommender agent using reinforcement learning. The project features a Streamlit interface for real-time visualization of user-recommender interactions and comprehensive evaluation metrics.

![alt text](assets/images/web_interface.png)

---

## ğŸ“Œ Project Status

âœ… **Implemented Components:**
- âœ… Product catalog with 250 diverse items (10 categories, multiple brands/colors)
- âœ… Six distinct user behavior models with realistic preferences
- âœ… Gymnasium-compatible environment for RL training
- âœ… Three recommender systems (Random, Popularity, RL-based)
- âœ… PPO-based RL agent with custom policy architecture
- âœ… Streamlit interface for interactive demonstrations
- âœ… Comprehensive evaluation framework with metrics tracking
- âœ… Weights & Biases integration for experiment tracking
- âœ… Training and evaluation scripts with CLI interface

---

## ğŸ§  Implemented Components

### Product Catalog
A generated catalog of **250 items** with rich attributes:
- **Categories**: Home, Beauty, Sports, Books, Clothing, Electronics, Toys
- **Attributes**: Price, quality score, brand, color, popularity, release date
- **Brands**: 15 different brands (BrandA through BrandO)
- **Colors**: White, Blue, Red, Green, Yellow, Black

### Simulated Users (6 Implemented Types)
- **CheapSeekerUser**: Prefers low-priced items with price-based utility
- **BrandLoverUser**: Has specific brand and color preferences  
- **ValueOptimizerUser**: Balances price and quality for optimal value
- **RandomChooserUser**: Makes random decisions with configurable noise
- **FamiliaritySeekerUser**: Requires repeated exposure before showing interest
- **FreshnessLookerUser**: Prefers newer items based on release date recency

### Recommender Systems
- **RandomRecommender**: Baseline random selection
- **PopularityRecommender**: Recommends based on item popularity scores
- **RLRecommender**: PPO-based agent with custom multi-input policy and embedding layers

### Interactive Environment
- **ShopEnv**: Gymnasium-compatible environment with:
  - Multi-user support with user embeddings
  - Rich observation space (user profile, item features, interaction history)
  - Reward based on click-through and buy-through rates
  - Configurable episode termination criteria

### Streamlit Interface
Real-time visualization featuring:
- User type and recommender selection
- Live recommendation display with user reactions
- Performance metrics (CTR, BTR, rewards)
- Session state management and environment reset

---

## ğŸ” Agent Interaction Flow

1. **Environment Setup**: User type and recommender are selected
2. **Recommendation**: Agent suggests items from candidate pool
3. **User Reaction**: Simulated user clicks/buys based on utility function
4. **Reward Calculation**: System computes reward from CTR/BTR metrics
5. **State Update**: Environment updates history and candidate pool
6. **Learning**: RL agent adapts policy based on accumulated experience

---

## ğŸ›  Installation & Setup

```bash
git clone https://github.com/ernestknurov/simshop.git
cd simshop

# Install dependencies with uv (recommended)
uv sync

# Or with pip
pip install -e .
```

---

## ğŸš€ Usage

### Run Streamlit Interface
```bash
make run
# or
uv run python -m streamlit run src/interface/app.py
```

### Train RL Model
```bash
make train ARGS="--total-timesteps 100000 --save-model-path models/my_model.zip"
# or
uv run python -m src.scripts.train --total-timesteps 100000
```

### Evaluate Models
```bash
make evaluate ARGS="--rl-model-path models/ppo_latest.zip --eval-episodes 1000"
# or
uv run python -m src.scripts.evaluate --rl-model-path models/ppo_latest.zip
```

---

## ğŸ“ Current Project Structure

```
simshop/
â”œâ”€â”€ pyproject.toml             # Dependencies and package metadata
â”œâ”€â”€ Makefile                   # Common commands (run, train, evaluate)
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ uv.lock                    # Dependency lock file
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ encoders.py        # Item encoding utilities
â”‚   â”œâ”€â”€ env/
â”‚   â”‚   â””â”€â”€ interaction_env.py # Gymnasium environment
â”‚   â”œâ”€â”€ interface/
â”‚   â”‚   â””â”€â”€ app.py             # Streamlit application
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ policies.py        # Custom RL policies
â”‚   â”œâ”€â”€ recommenders/
â”‚   â”‚   â”œâ”€â”€ baseline.py        # Random & Popularity recommenders
â”‚   â”‚   â””â”€â”€ advanced.py        # RL-based recommender
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ train.py           # Training script with W&B integration
â”‚   â”‚   â””â”€â”€ evaluate.py        # Evaluation script
â”‚   â”œâ”€â”€ users/
â”‚   â”‚   â”œâ”€â”€ base.py            # Abstract User class
â”‚   â”‚   â”œâ”€â”€ cheap_seeker.py    # Price-sensitive user
â”‚   â”‚   â”œâ”€â”€ brand_lover.py     # Brand-preference user
â”‚   â”‚   â”œâ”€â”€ value_optimizer.py # Quality-price balanced user
â”‚   â”‚   â”œâ”€â”€ random_chooser.py  # Random decision user
â”‚   â”‚   â”œâ”€â”€ familiarity_seeker.py # Repeat-exposure user
â”‚   â”‚   â””â”€â”€ freshness_looker.py # Recency-preference user
â”‚   â””â”€â”€ utils/                 # Helper functions
â”‚       â”œâ”€â”€ utils.py           # General utilities
â”‚       â”œâ”€â”€ callbacks.py       # W&B callbacks for logging
â”‚       â””â”€â”€ logger.py          # logger setup
â”‚
â””â”€â”€ notebooks/                 # Jupyter analysis notebooks
```

---

## âš™ï¸ Tech Stack

- **Python 3.12+** with modern dependency management (uv)
- **Gymnasium** for RL environment interface
- **Stable-Baselines3** for PPO implementation
- **Streamlit** for interactive web interface
- **PyTorch** for neural network components
- **Pandas/NumPy** for data manipulation
- **Weights & Biases** for experiment tracking
- **Faker** for synthetic data generation

---

## ğŸ”¬ Research Features

- **Multi-user RL training**: Simultaneous training across diverse user types
- **Custom policy architecture**: Top-K selection with embedding layers
- **Comprehensive metrics**: CTR, BTR, reward tracking with statistical analysis
- **Behavioral modeling**: Realistic user preferences with noise and memory
- **Experiment tracking**: Full W&B integration with model artifacts

---

## ğŸ“ˆ Future Enhancements

- [ ] More sophisticated user models (seasonal preferences, fatigue)
- [ ] Multi-armed bandit baselines for comparison
- [ ] Advanced RL algorithms (A2C, SAC, DQN variants)
- [ ] Cold-start problem simulation
- [ ] Real-world dataset integration
- [ ] A/B testing framework

---

## ğŸ‘¤ Author

Ernest Knurov  
ML Engineer & RL Enthusiast  
[GitHub](https://github.com/ernestknurov) â€¢ [LinkedIn](https://linkedin.com/in/ernestknurov)

---

## ğŸ“ License

MIT License
